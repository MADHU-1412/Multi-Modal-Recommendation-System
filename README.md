# Multi-Modal Recommendation System ðŸš€

**Goal:**  
Build a scalable, multi-modal recommendation engine that leverages **text, visual & metadata** from video content to predict user ratings â€” improving accuracy, interpretability, and usability in real-world recommender systems.

---

## ðŸ“Œ What I Achieved 

| # | Achievement | Key Outcome |
|---|------------|-------------|
| 1 | **Data Enrichment & Pre-processing** | Collected and cleaned enhanced MovieLens-100K dataset by scraping video trailers, extracting frames & summaries. Prepared modality-specific embeddings (visual, textual, metadata). |
| 2 | **Model Design** | Developed a Siamese architecture with multi-modal attention mechanism, enabling joint learning across modalities and metric-based user-item similarity. |
| 3 | **Training & Results** | Trained model end-to-end: achieved competitive RMSE/MAE and enhanced recommendation recall compared to baseline collaborative filtering. Saved model checkpoints & evaluation artifacts. |
| 4 | **Reproducibility & Developer Experience** | Provided full code base (notebook + scripts), environment setup, documentation and quickstart instructions for effortless reviewer evaluation. |

---



